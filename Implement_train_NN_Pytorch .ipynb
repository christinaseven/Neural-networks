{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use mnist dataset as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### steps\n",
    "1. download dataset\n",
    "2. create data loader\n",
    "3. build model\n",
    "4. train\n",
    "5. save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1\n",
    "def download_mnist_datasets():\n",
    "    train_data = datasets.MNIST(\n",
    "    root =\"data\", #tell pytorch where to store the data, it will store them in folder data\n",
    "    download = True,\n",
    "    train = True,\n",
    "    transform = ToTensor() #ToTensor takes the image n and reshapes a new tensor where each value is normalized between 0 and 1)\n",
    "    )\n",
    "    validation_data = datasets.MNIST(\n",
    "    root =\"data\", #tell pytorch where to store the data, it will store them in folder data\n",
    "    download = True,\n",
    "    train = False,\n",
    "    transform = ToTensor())\n",
    "    return train_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data = download_mnist_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2 : Dataloader for the train set\n",
    "#Dataloader is a class we can use to wrap a data set, in our case the train \n",
    "#data and it allows us to load data in batches. So it allows us that are very \n",
    "#heavy in memory!!\n",
    "BATCH_SIZE=128\n",
    "train_data_loader = DataLoader(train_data, batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3: build model/class\n",
    "#to define a model in Pytorch we have to define 2 methods: the constructor where we define\n",
    "#all the layers\n",
    "#and method forward to describe the network\n",
    "\n",
    "class FeedForwardNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__() #consructor of base class\n",
    "        self.flatten = nn.Flatten() #first layer is called flatten, it can have any name\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            #sequential allows us to pack together multiple layers and the data flow sequentialy from one level to the next\n",
    "            nn.Linear(28*28, 256), #the first dense layer, 256 neurons, 28x28 images\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,10) # we have 10 classes)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,input_data): #it indicates pytorch how to manipulate the data\n",
    "        flattened_data = self.flatten(input_data) #we pass input data to the flatten data and take x\n",
    "        logits = self.dense_layers(flattened_data) #logits are outputs\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    #if gpu accelaration is available\n",
    "    device =\"cuda\"\n",
    "else:\n",
    "    device =\"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward_net = FeedForwardNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model: we will use 2 functions. In train we'll go through all the epochs\n",
    "#that we want to train the model for and we'll call at each pass train_one_epoch\n",
    "def train_one_epoch(model, data_loader, loss_fn, optimiser, device):\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        #calculate loss\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        #backpropagate loss and update weights\n",
    "        optimiser.zero_grad() #the gradients at each itteration gets saved, we want at\n",
    "        #each train itteration to reset to zero(zero grad) to start from scratch\n",
    "        loss.backward()\n",
    "        optimiser.step() #update the weights\n",
    "    print(f\"Loss: {loss.item()}\") #print the loss for the last batch that we have\n",
    "    \n",
    "    \n",
    "def train(model, data_loader, loss_fn, optimiser, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"epoch {i+1}\")\n",
    "        train_one_epoch(model, data_loader, loss_fn, optimiser, device)\n",
    "        print(\"----------------\")\n",
    "    print(\"Trainig is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Loss: 1.5148301124572754\n",
      "----------------\n",
      "epoch 2\n",
      "Loss: 1.497542381286621\n",
      "----------------\n",
      "epoch 3\n",
      "Loss: 1.4873720407485962\n",
      "----------------\n",
      "epoch 4\n",
      "Loss: 1.4819507598876953\n",
      "----------------\n",
      "epoch 5\n",
      "Loss: 1.473847508430481\n",
      "----------------\n",
      "epoch 6\n",
      "Loss: 1.472737431526184\n",
      "----------------\n",
      "epoch 7\n",
      "Loss: 1.47238028049469\n",
      "----------------\n",
      "epoch 8\n",
      "Loss: 1.4723714590072632\n",
      "----------------\n",
      "epoch 9\n",
      "Loss: 1.4745192527770996\n",
      "----------------\n",
      "epoch 10\n",
      "Loss: 1.4726325273513794\n",
      "----------------\n",
      "Trainig is done.\n",
      "model trained and stored at feedforwardnet.pth\n"
     ]
    }
   ],
   "source": [
    "#train the model \n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(feed_forward_net.parameters(), lr =LEARNING_RATE)\n",
    "train(feed_forward_net, train_data_loader, loss_fn, optimiser, device, EPOCHS)\n",
    "\n",
    "#we want to store the model that we train!\n",
    "torch.save(feed_forward_net.state_dict(),\"feedforwardnet.pth\")\n",
    "print(\"model trained and stored at feedforwardnet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will make predictions with the model we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load back the model\n",
    "feed_forward_net = FeedForwardNet()\n",
    "state_dict = torch.load(\"feedforwardnet.pth\")\n",
    "feed_forward_net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input, target, class_mapping):\n",
    "    model.eval() #eval is like a switch that changes, if we activate eval certain layers turn of because we dont need them for inference\n",
    "    with torch.no_grad(): #not need to calculate gradients for inference, just for training\n",
    "        predictions = model(input)\n",
    "        #predictions is a Tensor with dimention (number of samples passing in model x number of classes we try to predict) =(1x10)\n",
    "        #Tensor (1,10) ---> [ [0.1, 0.01, ... , 0.6]] the sum will be 1 because of softmax\n",
    "        predicted_index = predictions[0].argmax(0)\n",
    "        predicted = class_mapping[predicted_index]\n",
    "        expected = class_mapping[target]\n",
    "    return predicted, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a sample from the validation dataset for inference\n",
    "input, target = validation_data[0][0], validation_data[0][1]\n",
    "#make an inference\n",
    "class_mapping = [\n",
    "    \"0\",\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\"\n",
    "]\n",
    "\n",
    "predicted, expected = predict(feed_forward_net, input, target, class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: '7', expected: '7'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted: '{predicted}', expected: '{expected}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
